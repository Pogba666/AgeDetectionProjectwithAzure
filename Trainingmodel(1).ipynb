{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trainingmodel(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCEiEYy73Dp0iePeJUVj4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjaySriram99/AgeDetectionProjectwithAzure/blob/main/Trainingmodel(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWUIGKI1d9LR",
        "outputId": "ca4df551-c979-43c0-9c90-69e043c53f1f"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mck2F-k3fTxS",
        "outputId": "8630be74-307c-4537-d69b-17e0377f2b4b"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib\r\n",
        "matplotlib.use('Agg')\r\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder # Label encoding, 1-hot encoding, multi-encoding\r\n",
        "# LABEL binarizer is a 1-hot encoded MATRIX \r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import imutils\r\n",
        "from imutils import paths\r\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array\r\n",
        "from keras.utils import to_categorical\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMMA24oxfWf5",
        "outputId": "c093863b-b112-4735-9a8a-0cec670faaad"
      },
      "source": [
        "!ls data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adolescent  college  middle_age  newborn  oldage  young_adult  young_child\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp0h0gebfZX_"
      },
      "source": [
        "HP_dataset = 'data'\r\n",
        "HP_model_path = 'bin/model'\r\n",
        "HP_binarized_labels = 'bin/labels'\r\n",
        "HP_metrics_storage = 'eval'\r\n",
        "HP_test_dataset = 'test'\r\n",
        "HP_epoch = 100\r\n",
        "HP_init_lr = 5e-3 # learning_rate = 0.001\r\n",
        "HP_batch_size = 16\r\n",
        "HP_image_dim = (48,48,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGkFgv8If-Na"
      },
      "source": [
        "data = []\r\n",
        "labels = [] \r\n",
        "# read all images\r\n",
        "all_images = sorted(list(paths.list_images(HP_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPOD-NhogCry"
      },
      "source": [
        "import os\r\n",
        "for impath in all_images:\r\n",
        "  img = cv2.imread(impath)\r\n",
        "  resized = cv2.resize(img, (HP_image_dim[0],HP_image_dim[1]) )\r\n",
        "  imageData = img_to_array(resized)\r\n",
        "  data.append(imageData)\r\n",
        "  # extract label from filename (2nd last element) / \\\\ \r\n",
        "  label = impath.split(os.path.sep)[-2]\r\n",
        "  labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S9P8mikgTeV"
      },
      "source": [
        "data = np.array(data, dtype='float' )/255.\r\n",
        "labels = np.array(labels)\r\n",
        "le = LabelEncoder()\r\n",
        "labels1 = le.fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2pfyDzsglzL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "trainx,testx,trainy,testy = train_test_split(data, labels1, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_kIGiEH8rEW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQu89ltj9fu"
      },
      "source": [
        "# Augmentation\r\n",
        "aug = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, \r\n",
        "                         height_shift_range=0.2, shear_range=0.2,\r\n",
        "                         zoom_range=0.2, horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdb29IblkAhp"
      },
      "source": [
        "from keras import backend\r\n",
        "from keras.layers.core import Dense, Dropout, Flatten, Activation\r\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import Adam\r\n",
        "#N= 5\r\n",
        "HP_block1_conv_dim = 32\r\n",
        "HP_block2_conv_dim = 64\r\n",
        "HP_block3_conv_dim = 128\r\n",
        "HP_block4_conv_dim = 256\r\n",
        "HP_block5_dense_dim = 1024\r\n",
        "HP_small_pattern = (3,3) # UNCOMPRESSED or 1-2 compression IMAGES\r\n",
        "HP_large_pattern = (2,2) # 4 times compressed images from previous MP layers!!!\r\n",
        "HP_dropout_little =0.20\r\n",
        "HP_dropout_big = 0.50\r\n",
        "# HP_epochs, batch_size-> are now problems of the developer USING this model. \r\n",
        "\r\n",
        "HP_img_dims = (48,48,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEqERQ2ESSzs"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\r\n",
        "from keras import layers, Model\r\n",
        "\r\n",
        "base_model = VGG16(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\r\n",
        "\r\n",
        "for layer in base_model.layers:\r\n",
        "    layer.trainable = False\r\n",
        "\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "\r\n",
        "# Flatten the output layer to 1 dimension\r\n",
        "x = layers.Flatten()(base_model.output)\r\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\r\n",
        "x = layers.Dense(1024, activation='relu')(x)\r\n",
        "# Add a dropout rate of 0.2\r\n",
        "x = layers.Dropout(0.2)(x)                  \r\n",
        "# Add a final softmax layer for classification\r\n",
        "x = layers.Dense(7, activation='softmax')(x)           \r\n",
        "\r\n",
        "model = Model( base_model.input, x) \r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSKj4pn8w1sf"
      },
      "source": [
        "base_model.compile(optimizer = RMSprop(lr=0.0001), \r\n",
        "              loss = 'sparse_categorical_crossentropy', \r\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZc2OD5ExU92"
      },
      "source": [
        "hist = base_model.fit(aug.flow(trainx, trainy, batch_size=HP_batch_size),\r\n",
        "                 validation_data=(testx, testy),\r\n",
        "                 steps_per_epoch= len(trainx)//HP_batch_size,\r\n",
        "                 epochs=HP_epoch, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tytn_QSVkDiL"
      },
      "source": [
        "class RacoonVGG:\r\n",
        "  @staticmethod\r\n",
        "  def build(height, width, depth,classes):\r\n",
        "    # assume that we are on TF, but if something else is detected, switch the dimension\r\n",
        "    input_shape = (height, width, depth)\r\n",
        "    channel_dim = -1 # last position \r\n",
        "    if backend.image_data_format() == 'channels_first':\r\n",
        "      input_shape = (depth, height, width)\r\n",
        "      channel_dim = 1\r\n",
        "    model = Sequential()\r\n",
        "    \r\n",
        "    # BLOCK1\r\n",
        "    model.add(Conv2D(HP_block1_conv_dim,HP_small_pattern, padding='same',\r\n",
        "                     input_shape=input_shape))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(MaxPooling2D(pool_size=HP_small_pattern))\r\n",
        "    model.add(Dropout(HP_dropout_little))\r\n",
        "\r\n",
        "\r\n",
        "    # COMPLEX BLOCK 2\r\n",
        "    model.add(Conv2D(HP_block2_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(Conv2D(HP_block2_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\r\n",
        "    model.add(Dropout(HP_dropout_little))\r\n",
        "    \r\n",
        "    # COMPLEX BLOCK 3\r\n",
        "    model.add(Conv2D(HP_block3_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(Conv2D(HP_block3_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\r\n",
        "    model.add(Dropout(HP_dropout_little))\r\n",
        "\r\n",
        "    # COMPLEX BLOCK 4\r\n",
        "    model.add(Conv2D(HP_block4_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(Conv2D(HP_block4_conv_dim,HP_small_pattern, padding='same'))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization(axis=channel_dim))\r\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\r\n",
        "    model.add(Dropout(HP_dropout_little))\r\n",
        "\r\n",
        "    # BLOCK 5- Image Classification (OBJECT)\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(HP_block5_dense_dim))\r\n",
        "    model.add(Activation('relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(HP_dropout_big))\r\n",
        "    model.add(Dense(classes))\r\n",
        "    model.add(Activation('softmax'))\r\n",
        "\r\n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVIzrHrV-0QO"
      },
      "source": [
        "# from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "model = RacoonVGG.build(height=HP_img_dims[0], width=HP_img_dims[1], depth=HP_img_dims[2],classes = len(le.classes_))\r\n",
        "opt = Adam(lr=HP_init_lr, decay = HP_init_lr/ HP_epoch)\r\n",
        "model.compile(loss= 'sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSdjVJkhkbzf",
        "outputId": "d09cd660-7e00-43be-b35f-904d165f5647"
      },
      "source": [
        "hist = model.fit(aug.flow(trainx, trainy, batch_size=HP_batch_size),\r\n",
        "                 validation_data=(testx, testy),\r\n",
        "                 steps_per_epoch= len(trainx)//HP_batch_size,\r\n",
        "                 epochs=HP_epoch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 19s 261ms/step - loss: 3.3427 - accuracy: 0.1667 - val_loss: 74.9965 - val_accuracy: 0.1270\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 3.2319 - accuracy: 0.1883 - val_loss: 13.6799 - val_accuracy: 0.1329\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.9226 - accuracy: 0.1813 - val_loss: 4.2092 - val_accuracy: 0.1806\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.9454 - accuracy: 0.1770 - val_loss: 6.8294 - val_accuracy: 0.1468\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 3.0609 - accuracy: 0.1727 - val_loss: 10.9939 - val_accuracy: 0.1349\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.8489 - accuracy: 0.1693 - val_loss: 2.9512 - val_accuracy: 0.1468\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.9765 - accuracy: 0.1744 - val_loss: 2.9126 - val_accuracy: 0.1488\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 18s 247ms/step - loss: 2.6235 - accuracy: 0.2124 - val_loss: 2.3525 - val_accuracy: 0.1865\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.7240 - accuracy: 0.1718 - val_loss: 2.1524 - val_accuracy: 0.1250\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.5748 - accuracy: 0.1978 - val_loss: 3.1699 - val_accuracy: 0.1290\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.6904 - accuracy: 0.1813 - val_loss: 8.6453 - val_accuracy: 0.1290\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 18s 247ms/step - loss: 2.4619 - accuracy: 0.1917 - val_loss: 2.5412 - val_accuracy: 0.1349\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 18s 247ms/step - loss: 2.4206 - accuracy: 0.1788 - val_loss: 4.3313 - val_accuracy: 0.1845\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 18s 247ms/step - loss: 2.5077 - accuracy: 0.1641 - val_loss: 2.4763 - val_accuracy: 0.2103\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 18s 247ms/step - loss: 2.3500 - accuracy: 0.1934 - val_loss: 2.0315 - val_accuracy: 0.2103\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.3245 - accuracy: 0.2003 - val_loss: 2.2030 - val_accuracy: 0.2163\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.2531 - accuracy: 0.1813 - val_loss: 3.7134 - val_accuracy: 0.1786\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.1982 - accuracy: 0.2064 - val_loss: 1.9929 - val_accuracy: 0.2024\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.3085 - accuracy: 0.1857 - val_loss: 2.1905 - val_accuracy: 0.1190\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 2.2438 - accuracy: 0.2124 - val_loss: 2.0170 - val_accuracy: 0.2520\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 2.1993 - accuracy: 0.2081 - val_loss: 1.9495 - val_accuracy: 0.1984\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.2098 - accuracy: 0.2175 - val_loss: 2.7426 - val_accuracy: 0.2063\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.1221 - accuracy: 0.2211 - val_loss: 4.3501 - val_accuracy: 0.1845\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 2.1587 - accuracy: 0.2142 - val_loss: 2.4471 - val_accuracy: 0.1746\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.1413 - accuracy: 0.2247 - val_loss: 2.1889 - val_accuracy: 0.2282\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 2.2102 - accuracy: 0.2090 - val_loss: 2.5367 - val_accuracy: 0.1845\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 2.0590 - accuracy: 0.2106 - val_loss: 2.6706 - val_accuracy: 0.2004\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 2.1095 - accuracy: 0.2047 - val_loss: 2.1264 - val_accuracy: 0.2500\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.9776 - accuracy: 0.2423 - val_loss: 2.0933 - val_accuracy: 0.2143\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 2.0098 - accuracy: 0.2496 - val_loss: 2.8812 - val_accuracy: 0.1687\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.9388 - accuracy: 0.2686 - val_loss: 1.8383 - val_accuracy: 0.2500\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 1.9700 - accuracy: 0.2178 - val_loss: 1.9121 - val_accuracy: 0.1885\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.9864 - accuracy: 0.2303 - val_loss: 1.9075 - val_accuracy: 0.2202\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 1.9354 - accuracy: 0.2500 - val_loss: 1.8912 - val_accuracy: 0.2381\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.9385 - accuracy: 0.2312 - val_loss: 1.8670 - val_accuracy: 0.2321\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 1.8950 - accuracy: 0.2631 - val_loss: 1.8581 - val_accuracy: 0.2321\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.9329 - accuracy: 0.2358 - val_loss: 1.9925 - val_accuracy: 0.2381\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8640 - accuracy: 0.2642 - val_loss: 2.1377 - val_accuracy: 0.2599\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8990 - accuracy: 0.2470 - val_loss: 1.8704 - val_accuracy: 0.2599\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8826 - accuracy: 0.2660 - val_loss: 2.2184 - val_accuracy: 0.2718\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8721 - accuracy: 0.2530 - val_loss: 1.9380 - val_accuracy: 0.2440\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.8613 - accuracy: 0.2642 - val_loss: 1.8436 - val_accuracy: 0.2361\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 19s 254ms/step - loss: 1.8676 - accuracy: 0.2791 - val_loss: 1.8190 - val_accuracy: 0.2937\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 1.8624 - accuracy: 0.2561 - val_loss: 1.8264 - val_accuracy: 0.2877\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.8572 - accuracy: 0.2543 - val_loss: 1.8264 - val_accuracy: 0.2778\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 1.8083 - accuracy: 0.2927 - val_loss: 1.8745 - val_accuracy: 0.2500\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.8380 - accuracy: 0.2834 - val_loss: 2.0808 - val_accuracy: 0.1944\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7993 - accuracy: 0.3005 - val_loss: 1.9074 - val_accuracy: 0.2639\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 1.8491 - accuracy: 0.2578 - val_loss: 1.8589 - val_accuracy: 0.2103\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.8119 - accuracy: 0.2997 - val_loss: 1.8523 - val_accuracy: 0.2698\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.8170 - accuracy: 0.2817 - val_loss: 1.8822 - val_accuracy: 0.2679\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7970 - accuracy: 0.2832 - val_loss: 1.8445 - val_accuracy: 0.2817\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8138 - accuracy: 0.2814 - val_loss: 1.8574 - val_accuracy: 0.2579\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.8051 - accuracy: 0.2834 - val_loss: 1.7810 - val_accuracy: 0.2857\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.8060 - accuracy: 0.2997 - val_loss: 1.8837 - val_accuracy: 0.2500\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.8079 - accuracy: 0.2919 - val_loss: 2.0761 - val_accuracy: 0.2024\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7661 - accuracy: 0.3135 - val_loss: 1.9117 - val_accuracy: 0.2778\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7944 - accuracy: 0.2884 - val_loss: 1.9100 - val_accuracy: 0.2440\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7567 - accuracy: 0.2962 - val_loss: 1.8998 - val_accuracy: 0.2401\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7549 - accuracy: 0.3178 - val_loss: 1.8404 - val_accuracy: 0.2937\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7657 - accuracy: 0.3169 - val_loss: 2.0009 - val_accuracy: 0.1905\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7748 - accuracy: 0.3100 - val_loss: 1.8054 - val_accuracy: 0.3016\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7611 - accuracy: 0.2997 - val_loss: 1.8688 - val_accuracy: 0.3194\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.8117 - accuracy: 0.3014 - val_loss: 1.8855 - val_accuracy: 0.2837\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7975 - accuracy: 0.2651 - val_loss: 2.0585 - val_accuracy: 0.1667\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.8041 - accuracy: 0.2945 - val_loss: 1.8320 - val_accuracy: 0.2897\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7688 - accuracy: 0.3022 - val_loss: 1.8110 - val_accuracy: 0.3155\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7729 - accuracy: 0.3048 - val_loss: 1.7745 - val_accuracy: 0.2976\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 1.7901 - accuracy: 0.2988 - val_loss: 1.8355 - val_accuracy: 0.2897\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 19s 254ms/step - loss: 1.7565 - accuracy: 0.3057 - val_loss: 1.7677 - val_accuracy: 0.3353\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7613 - accuracy: 0.3083 - val_loss: 1.7641 - val_accuracy: 0.3155\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7529 - accuracy: 0.3005 - val_loss: 1.8103 - val_accuracy: 0.2897\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7623 - accuracy: 0.3161 - val_loss: 1.7835 - val_accuracy: 0.3175\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7292 - accuracy: 0.3195 - val_loss: 1.8444 - val_accuracy: 0.2778\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 18s 248ms/step - loss: 1.7617 - accuracy: 0.3031 - val_loss: 1.7874 - val_accuracy: 0.3075\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7340 - accuracy: 0.3299 - val_loss: 1.8934 - val_accuracy: 0.2520\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7218 - accuracy: 0.3351 - val_loss: 2.0642 - val_accuracy: 0.2897\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7853 - accuracy: 0.3005 - val_loss: 1.8192 - val_accuracy: 0.3095\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7277 - accuracy: 0.3256 - val_loss: 2.2842 - val_accuracy: 0.2321\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 18s 249ms/step - loss: 1.7916 - accuracy: 0.2971 - val_loss: 1.9854 - val_accuracy: 0.2718\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7502 - accuracy: 0.3187 - val_loss: 1.9391 - val_accuracy: 0.2500\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7467 - accuracy: 0.3282 - val_loss: 1.7889 - val_accuracy: 0.2698\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7616 - accuracy: 0.3057 - val_loss: 2.4758 - val_accuracy: 0.2004\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7570 - accuracy: 0.3299 - val_loss: 1.8439 - val_accuracy: 0.2778\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7433 - accuracy: 0.3117 - val_loss: 1.7842 - val_accuracy: 0.2917\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.7592 - accuracy: 0.2936 - val_loss: 1.9634 - val_accuracy: 0.2560\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7501 - accuracy: 0.3307 - val_loss: 1.7726 - val_accuracy: 0.2897\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7662 - accuracy: 0.3187 - val_loss: 1.8622 - val_accuracy: 0.2817\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7489 - accuracy: 0.2902 - val_loss: 1.9804 - val_accuracy: 0.2341\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7218 - accuracy: 0.3356 - val_loss: 1.8015 - val_accuracy: 0.3016\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7535 - accuracy: 0.3197 - val_loss: 1.7727 - val_accuracy: 0.3115\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 18s 251ms/step - loss: 1.6947 - accuracy: 0.3428 - val_loss: 1.8053 - val_accuracy: 0.3155\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7269 - accuracy: 0.3279 - val_loss: 1.8450 - val_accuracy: 0.2500\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.7371 - accuracy: 0.3206 - val_loss: 1.9253 - val_accuracy: 0.2857\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7048 - accuracy: 0.3411 - val_loss: 2.0031 - val_accuracy: 0.2698\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 18s 253ms/step - loss: 1.7327 - accuracy: 0.3264 - val_loss: 1.8564 - val_accuracy: 0.2956\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.7156 - accuracy: 0.3472 - val_loss: 1.9278 - val_accuracy: 0.2877\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 18s 252ms/step - loss: 1.6983 - accuracy: 0.3411 - val_loss: 1.7985 - val_accuracy: 0.3135\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 19s 254ms/step - loss: 1.7104 - accuracy: 0.3476 - val_loss: 1.8114 - val_accuracy: 0.2778\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 18s 250ms/step - loss: 1.6614 - accuracy: 0.3554 - val_loss: 1.9144 - val_accuracy: 0.2817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjyqZolktNvw"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olLfZoMTZVG",
        "outputId": "d643b57b-fc90-4082-b30b-554748876054"
      },
      "source": [
        "pre_trained_model = InceptionV3(input_shape = (224, 224, 3), # Shape of our images\r\n",
        "                                include_top = False, # Leave out the last fully connected layer\r\n",
        "                                weights = 'imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJifuvEaTgXs"
      },
      "source": [
        "for layer in pre_trained_model.layers:\r\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "_HFB-hm7TjP1",
        "outputId": "cbaa77ce-5bc0-4da9-d46f-df234f2e8f6c"
      },
      "source": [
        "from keras.optimizers import RMSprop\r\n",
        "\r\n",
        "# Flatten the output layer to 1 dimension\r\n",
        "x = layers.Flatten()(pre_trained_model.output)\r\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\r\n",
        "x = layers.Dense(1024, activation='relu')(x)\r\n",
        "# Add a dropout rate of 0.2\r\n",
        "x = layers.Dropout(0.2)(x)                  \r\n",
        "# Add a final sigmoid layer for classification\r\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \r\n",
        "\r\n",
        "model = Model( pre_trained_model.input, x) \r\n",
        "\r\n",
        "pre_trained_model.compile(optimizer = RMSprop(lr=0.0001), \r\n",
        "              loss = 'sparse_categorical_crossentropy', \r\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b630b2e9c5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Flatten the output layer to 1 dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'reshape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "gSja3Y3WTQtu",
        "outputId": "cef4c98f-6df1-489a-b34e-7b35bd732f36"
      },
      "source": [
        "hist = pre_trained_model.fit(aug.flow(trainx, trainy, batch_size=HP_batch_size),\r\n",
        "                 validation_data=(testx, testy),\r\n",
        "                 steps_per_epoch= len(trainx)//HP_batch_size,   #// batch_size for larger datasets\r\n",
        "                 epochs=HP_epoch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fec71a7b3673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mHP_batch_size\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m#// batch_size for larger datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                  epochs=HP_epoch, verbose=1)\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                 initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                      str(validation_data))\n\u001b[1;32m    149\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 150\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected mixed10 to have 4 dimensions, but got array with shape (336, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmXdJHp1UjCq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}